@inproceedings{agnolucci2024arniqa,
  abbr={WACV},
  bibtex_show={true},
  selected={true},
  title={ARNIQA: Learning Distortion Manifold for Image Quality Assessment},
  author={Agnolucci, L. and Galteri, L. and Bertini, M. and Del Bimbo, A.},
  abstract={No-Reference Image Quality Assessment (NR-IQA) aims to develop methods to measure image quality in alignment with human perception without the need for a high-quality reference image. In this work, we propose a self-supervised approach named ARNIQA (leArning distoRtion maNifold for Image Quality Assessment) for modeling the image distortion manifold to obtain quality representations in an intrinsic manner. First, we introduce an image degradation model that randomly composes ordered sequences of consecutively applied distortions. In this way, we can synthetically degrade images with a large variety of degradation patterns. Second, we propose to train our model by maximizing the similarity between the representations of patches of different images distorted equally, despite varying content. Therefore, images degraded in the same manner correspond to neighboring positions within the distortion manifold. Finally, we map the image representations to the quality scores with a simple linear regressor, thus without fine-tuning the encoder weights. The experiments show that our approach achieves state-of-the-art performance on several datasets. In addition, ARNIQA demonstrates improved data efficiency, generalization capabilities, and robustness compared to competing methods. The code and the model are publicly available at https://github.com/miccunifi/ARNIQA.},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  year={2024},
  arxiv={2310.14918},
  code={https://github.com/miccunifi/ARNIQA},
  pdf={arniqa_arxiv.pdf},
}

@inproceedings{agnolucci2024reference,
  abbr={WACV},
  bibtex_show={true},
  title={Reference-based Restoration of Digitized Analog Videotapes},
  author={Agnolucci, L. and Galteri, L. and Bertini, M. and Del Bimbo, A.},
  abstract={Analog magnetic tapes have been the main video data storage device for several decades. Videos stored on analog videotapes exhibit unique degradation patterns caused by tape aging and reader device malfunctioning that are different from those observed in film and digital video restoration tasks. In this work, we present a reference-based approach for the resToration of digitized Analog videotaPEs (TAPE). We leverage CLIP for zero-shot artifact detection to identify the cleanest frames of each video through textual prompts describing different artifacts. Then, we select the clean frames most similar to the input ones and employ them as references. We design a transformer-based Swin-UNet network that exploits both neighboring and reference frames via our Multi-Reference Spatial Feature Fusion (MRSFF) blocks. MRSFF blocks rely on cross-attention and attention pooling to take advantage of the most useful parts of each reference frame. To address the absence of ground truth in real-world videos, we create a synthetic dataset of videos exhibiting artifacts that closely resemble those commonly found in analog videotapes. Both quantitative and qualitative experiments show the effectiveness of our approach compared to other state-of-the-art methods. The code, the model, and the synthetic dataset are publicly available at https://github.com/miccunifi/TAPE.},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  year={2024},
  arxiv={2310.14926},
  code={https://github.com/miccunifi/TAPE},
  pdf={tape_arxiv.pdf},
}

@inproceedings{baldrati2023zero,
  abbr={ICCV},
  bibtex_show={true},
  selected={true},
  title={Zero-Shot Composed Image Retrieval with Textual Inversion},
  author={Baldrati*, A. and Agnolucci*, L. and Bertini, M. and Del Bimbo, A.},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  abstract={Composed Image Retrieval (CIR) aims to retrieve a target image based on a query composed of a reference image and a relative caption that describes the difference between the two images. The high effort and cost required for labeling datasets for CIR hamper the widespread usage of existing methods, as they rely on supervised learning. In this work, we propose a new task, Zero-Shot CIR (ZS-CIR), that aims to address CIR without requiring a labeled training dataset. Our approach, named zero-Shot composEd imAge Retrieval with textuaL invErsion (SEARLE), maps the visual features of the reference image into a pseudo-word token in CLIP token embedding space and integrates it with the relative caption. To support research on ZS-CIR, we introduce an open-domain benchmarking dataset named Composed Image Retrieval on Common Objects in context (CIRCO), which is the first dataset for CIR containing multiple ground truths for each query. The experiments show that SEARLE exhibits better performance than the baselines on the two main datasets for CIR tasks, FashionIQ and CIRR, and on the proposed CIRCO. The dataset, the code and the model are publicly available at https://github.com/miccunifi/SEARLE.},
  pages={15338--15347},
  year={2023},
  arxiv={2303.15247},
  code={https://github.com/miccunifi/SEARLE},
  website={https://circo.micc.unifi.it/demo},
  pdf={searle_iccv2023.pdf},
}

@inproceedings{agnolucci2023eco,
  abbr={ICCV},
  abbrtype={Workshop},
  bibtex_show={true},
  title={ECO: Ensembling Context Optimization for Vision-Language Models},
  author={Agnolucci*, L. and Baldrati*, A. and Todino, F. and Becattini, F. and Bertini, M. and Del Bimbo, A.},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  abstract={Image recognition has recently witnessed a paradigm shift, where vision-language models are now used to perform few-shot classification based on textual prompts. Among these, the CLIP model has shown remarkable capabilities for zero-shot transfer by matching an image and a custom textual prompt in its latent space. This has paved the way for several works that focus on engineering or learning textual contexts for maximizing CLIP's classification capabilities. In this paper, we follow this trend by learning an ensemble of prompts for image classification. We show that learning diverse and possibly shorter contexts improves considerably and consistently the results rather than relying on a single trainable prompt. In particular, we report better few-shot capabilities with no additional cost at inference time. We demonstrate the capabilities of our approach on 11 different benchmarks.},
  pages={2811--2815},
  year={2023},
  arxiv={2307.14063},
  %html={https://openaccess.thecvf.com/content/ICCV2023W/CLVL/html/Agnolucci_ECO_Ensembling_Context_Optimization_for_Vision-Language_Models_ICCVW_2023_paper.html},
  pdf={eco_workshop_iccv2023.pdf},
}

@inproceedings{burbi2023mapping,
  abbr={ICCV},
  abbrtype={Workshop},
  bibtex_show={true},
  title={Mapping Memes to Words for Multimodal Hateful Meme Classification},
  author={Burbi, G. and Baldrati, A. and Agnolucci, L. and Bertini, M. and Del Bimbo, A.},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  abstract={Multimodal image-text memes are prevalent on the internet, serving as a unique form of communication that combines visual and textual elements to convey humor, ideas, or emotions. However, some memes take a malicious turn, promoting hateful content and perpetuating discrimination. Detecting hateful memes within this multimodal context is a challenging task that requires understanding the intertwined meaning of text and images. In this work, we address this issue by proposing a novel approach named ISSUES for multimodal hateful meme classification. ISSUES leverages a pre-trained CLIP vision-language model and the textual inversion technique to effectively capture the multimodal semantic content of the memes. The experiments show that our method achieves state-of-the-art results on the Hateful Memes Challenge and HarMeme datasets. The code and the pre-trained models are publicly available at https://github.com/miccunifi/ISSUES.},
  pages={2832--2836},
  year={2023},
  arxiv={2310.08368},
  %html={https://openaccess.thecvf.com/content/ICCV2023W/CLVL/html/Burbi_Mapping_Memes_to_Words_for_Multimodal_Hateful_Meme_Classification_ICCVW_2023_paper.html},
  code={https://github.com/miccunifi/ISSUES},
  pdf={issues_workshop_iccv2023.pdf}
}

@article{agnolucci2023perceptual,
  abbr={IEEE TMM},
  bibtex_show={true},
  title={Perceptual Quality Improvement in Videoconferencing using Keyframes-based GAN},
  author={Agnolucci, L. and Galteri, L. and Bertini, M. and Del Bimbo, A.},
  journal={IEEE Transactions on Multimedia},
  year={2023},
  publisher={IEEE},
  abstract={In the latest years, videoconferencing has taken a fundamental role in interpersonal relations, both for personal and business purposes. Lossy video compression algorithms are the enabling technology for videoconferencing, as they reduce the bandwidth required for real-time video streaming. However, lossy video compression decreases the perceived visual quality. Thus, many techniques for reducing compression artifacts and improving video visual quality have been proposed in recent years. In this work, we propose a novel GAN-based method for compression artifacts reduction in videoconferencing. Given that, in this context, the speaker is typically in front of the camera and remains the same for the entire duration of the transmission, we can maintain a set of reference keyframes of the person from the higher-quality I-frames that are transmitted within the video stream and exploit them to guide the visual quality improvement; a novel aspect of this approach is the update policy that maintains and updates a compact and effective set of reference keyframes. First, we extract multi-scale features from the compressed and reference frames. Then, our architecture combines these features in a progressive manner according to facial landmarks. This allows the restoration of the high-frequency details lost after the video compression. Experiments show that the proposed approach improves visual quality and generates photo-realistic results even with high compression rates. Code and pre-trained networks are publicly available at https://github.com/LorenzoAgnolucci/Keyframes-GAN.},
  html={https://ieeexplore.ieee.org/abstract/document/10093128},
  code={https://github.com/LorenzoAgnolucci/Keyframes-GAN},
  pdf={videoconferencing_ieeetmm2023.pdf},
}

@inproceedings{agnolucci2022restoration,
  abbr={ACM MM},
  abbrtype={Demo},
  bibtex_show={true},
  title={Restoration of Analog Videos Using Swin-UNet},
  author={Agnolucci, L. and Galteri, L. and Bertini, M. and Del Bimbo, A.},
  booktitle={Proceedings of the 30th ACM International Conference on Multimedia},
  pages={6985--6987},
  year={2022},
  abstract={In this paper we present a system to restore analog videos of historical archives. These videos often contain severe visual degradation due to the deterioration of their tape supports that require costly and slow manual interventions to recover the original content. The proposed system uses a multi-frame approach and is able to deal also with severe tape mistracking, which results in completely scrambled frames. Tests on real-world videos from a major historical video archive show the effectiveness of our demo system.},
  html={https://dl.acm.org/doi/10.1145/3503161.3547730},
  code={https://github.com/miccunifi/analog-video-restoration},
  pdf={demo_acmmm2022.pdf},
}